{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### Q1. **Data** *Preprocessing*"
      ],
      "metadata": {
        "id": "XI52vTjC_F38"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from bs4 import BeautifulSoup\n",
        "import string\n",
        "import random"
      ],
      "metadata": {
        "id": "oq5In44KREGJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def download_nltk_resources():\n",
        "\n",
        "    nltk.download('punkt')\n",
        "    nltk.download('stopwords')\n",
        "\n",
        "def get_data_path():\n",
        "\n",
        "    data_path = \"/content/drive/MyDrive/text_files\"\n",
        "    return data_path"
      ],
      "metadata": {
        "id": "hYWEWV-rREJw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess(input_text, display_steps=True):\n",
        "    # Step 1: Remove HTML tags\n",
        "    soup_obj = BeautifulSoup(input_text, 'html.parser')\n",
        "    text = soup_obj.get_text()\n",
        "    if display_steps:\n",
        "        print(f\"Step 1: Remove HTML tags\\n{text}\\n\")\n",
        "\n",
        "    # Step 2: Lowercase the text\n",
        "    text = text.lower()\n",
        "    if display_steps:\n",
        "        print(f\"Step 2: Lowercase the text\\n{text}\\n\")\n",
        "\n",
        "    # Step 3: Tokenization\n",
        "    tokens = word_tokenize(text)\n",
        "    if display_steps:\n",
        "        print(f\"Step 3: Tokenization\\n{tokens}\\n\")\n",
        "\n",
        "    # Step 4: Remove stopwords\n",
        "    stop_words_set = set(stopwords.words('english'))\n",
        "    stop_words_tokens = [token for token in tokens if token.lower() not in stop_words_set]\n",
        "    if display_steps:\n",
        "        print(f\"Step 4: Remove stopwords\\n{stop_words_tokens}\\n\")\n",
        "\n",
        "    # Step 5: Remove punctuations\n",
        "    translator = str.maketrans('', '', string.punctuation)\n",
        "    tokens_without_punctuation = [token.translate(translator) for token in stop_words_tokens]\n",
        "    if display_steps:\n",
        "        print(f\"Step 5: Remove punctuations\\n{tokens_without_punctuation}\\n\")\n",
        "\n",
        "\n",
        "    filtered_tokens_without_blank_space = [token for token in tokens_without_punctuation if token.strip()]\n",
        "    preprocessed_text = ' '.join(filtered_tokens_without_blank_space)\n",
        "    if display_steps:\n",
        "        print(f\"Step 6: Remove blank space token\\n{preprocessed_text}\\n\")\n",
        "\n",
        "    return preprocessed_text"
      ],
      "metadata": {
        "id": "VtYfLsoQREMT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WK-dYNedREO5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "download_nltk_resources()\n",
        "data_path = get_data_path()\n",
        "\n",
        "\n",
        "preprocessed_data_path = os.path.join(data_path, \"preprocessed_files\")\n",
        "os.makedirs(preprocessed_data_path, exist_ok=True)\n",
        "\n",
        "text_files = [filename for filename in os.listdir(data_path) if filename.endswith('.txt')]\n",
        "random_sample_files = random.sample(text_files, 5)\n",
        "\n",
        "for filename in text_files:\n",
        "    file_path = os.path.join(data_path, filename)\n",
        "    with open(file_path, 'r', encoding='utf-8') as file:\n",
        "        original_text = file.read()\n",
        "\n",
        "    if filename in random_sample_files:\n",
        "        print(f\"\\nSample - File: {filename}\\n\")\n",
        "        preprocess(original_text)\n",
        "    else:\n",
        "        preprocessed_text = preprocess(original_text, display_steps=False)\n",
        "\n",
        "    new_filename = \"preprocessed_\" + filename\n",
        "    new_file_path = os.path.join(preprocessed_data_path, new_filename)\n",
        "    with open(new_file_path, 'w', encoding='utf-8') as new_file:\n",
        "        new_file.write(preprocessed_text)\n",
        "\n"
      ],
      "metadata": {
        "id": "vBg_K2YvtbZs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9006c755-71ce-40cc-9007-73c3053843e3"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "<ipython-input-48-bd6b1a087abb>:21: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
            "  soup_obj = BeautifulSoup(input_text, 'html.parser')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Sample - File: file109.txt\n",
            "\n",
            "Step 1: Remove HTML tags\n",
            "Odyssey 8-Space Rack is great. I put casters on mine to move it around when I need to.\n",
            "\n",
            "Step 2: Lowercase the text\n",
            "odyssey 8-space rack is great. i put casters on mine to move it around when i need to.\n",
            "\n",
            "Step 3: Tokenization\n",
            "['odyssey', '8-space', 'rack', 'is', 'great', '.', 'i', 'put', 'casters', 'on', 'mine', 'to', 'move', 'it', 'around', 'when', 'i', 'need', 'to', '.']\n",
            "\n",
            "Step 4: Remove stopwords\n",
            "['odyssey', '8-space', 'rack', 'great', '.', 'put', 'casters', 'mine', 'move', 'around', 'need', '.']\n",
            "\n",
            "Step 5: Remove punctuations\n",
            "['odyssey', '8space', 'rack', 'great', '', 'put', 'casters', 'mine', 'move', 'around', 'need', '']\n",
            "\n",
            "Step 6: Remove blank space token\n",
            "odyssey 8space rack great put casters mine move around need\n",
            "\n",
            "\n",
            "Sample - File: file458.txt\n",
            "\n",
            "Step 1: Remove HTML tags\n",
            "I have nothing bad to say about this stand. It works and it gets the job done. I use it at home,  and I have placed it in a corner of the room with 3 electric guitars, one acoustic, and one bass.\n",
            "\n",
            "The instructions were just a picture but I got it assembled in less than 10 minutes. Really easy to do.\n",
            "\n",
            "Great multi guitar rack/stand for the price. I definitely recommend.\n",
            "\n",
            "Step 2: Lowercase the text\n",
            "i have nothing bad to say about this stand. it works and it gets the job done. i use it at home,  and i have placed it in a corner of the room with 3 electric guitars, one acoustic, and one bass.\n",
            "\n",
            "the instructions were just a picture but i got it assembled in less than 10 minutes. really easy to do.\n",
            "\n",
            "great multi guitar rack/stand for the price. i definitely recommend.\n",
            "\n",
            "Step 3: Tokenization\n",
            "['i', 'have', 'nothing', 'bad', 'to', 'say', 'about', 'this', 'stand', '.', 'it', 'works', 'and', 'it', 'gets', 'the', 'job', 'done', '.', 'i', 'use', 'it', 'at', 'home', ',', 'and', 'i', 'have', 'placed', 'it', 'in', 'a', 'corner', 'of', 'the', 'room', 'with', '3', 'electric', 'guitars', ',', 'one', 'acoustic', ',', 'and', 'one', 'bass', '.', 'the', 'instructions', 'were', 'just', 'a', 'picture', 'but', 'i', 'got', 'it', 'assembled', 'in', 'less', 'than', '10', 'minutes', '.', 'really', 'easy', 'to', 'do', '.', 'great', 'multi', 'guitar', 'rack/stand', 'for', 'the', 'price', '.', 'i', 'definitely', 'recommend', '.']\n",
            "\n",
            "Step 4: Remove stopwords\n",
            "['nothing', 'bad', 'say', 'stand', '.', 'works', 'gets', 'job', 'done', '.', 'use', 'home', ',', 'placed', 'corner', 'room', '3', 'electric', 'guitars', ',', 'one', 'acoustic', ',', 'one', 'bass', '.', 'instructions', 'picture', 'got', 'assembled', 'less', '10', 'minutes', '.', 'really', 'easy', '.', 'great', 'multi', 'guitar', 'rack/stand', 'price', '.', 'definitely', 'recommend', '.']\n",
            "\n",
            "Step 5: Remove punctuations\n",
            "['nothing', 'bad', 'say', 'stand', '', 'works', 'gets', 'job', 'done', '', 'use', 'home', '', 'placed', 'corner', 'room', '3', 'electric', 'guitars', '', 'one', 'acoustic', '', 'one', 'bass', '', 'instructions', 'picture', 'got', 'assembled', 'less', '10', 'minutes', '', 'really', 'easy', '', 'great', 'multi', 'guitar', 'rackstand', 'price', '', 'definitely', 'recommend', '']\n",
            "\n",
            "Step 6: Remove blank space token\n",
            "nothing bad say stand works gets job done use home placed corner room 3 electric guitars one acoustic one bass instructions picture got assembled less 10 minutes really easy great multi guitar rackstand price definitely recommend\n",
            "\n",
            "\n",
            "Sample - File: file618.txt\n",
            "\n",
            "Step 1: Remove HTML tags\n",
            "I purchased these stands to hold my Yamaha HS5's. These stands are really well made..heavy and have a nice look to them. I really like the triangle base as it makes it easy for them to fit in the corners of my room. They have locking pins that are great for keeping the stands at the height that you set. They also have some type of rubber feet that helps with leveling the stand. These are pretty impressive. The only thing that I don't like is that the platform that the speakers sit on are a bit larger than i'd like...but this is more my fault for not reading the dimensions. Other than that, I highly recommend these.\n",
            "\n",
            "Step 2: Lowercase the text\n",
            "i purchased these stands to hold my yamaha hs5's. these stands are really well made..heavy and have a nice look to them. i really like the triangle base as it makes it easy for them to fit in the corners of my room. they have locking pins that are great for keeping the stands at the height that you set. they also have some type of rubber feet that helps with leveling the stand. these are pretty impressive. the only thing that i don't like is that the platform that the speakers sit on are a bit larger than i'd like...but this is more my fault for not reading the dimensions. other than that, i highly recommend these.\n",
            "\n",
            "Step 3: Tokenization\n",
            "['i', 'purchased', 'these', 'stands', 'to', 'hold', 'my', 'yamaha', 'hs5', \"'s\", '.', 'these', 'stands', 'are', 'really', 'well', 'made', '..', 'heavy', 'and', 'have', 'a', 'nice', 'look', 'to', 'them', '.', 'i', 'really', 'like', 'the', 'triangle', 'base', 'as', 'it', 'makes', 'it', 'easy', 'for', 'them', 'to', 'fit', 'in', 'the', 'corners', 'of', 'my', 'room', '.', 'they', 'have', 'locking', 'pins', 'that', 'are', 'great', 'for', 'keeping', 'the', 'stands', 'at', 'the', 'height', 'that', 'you', 'set', '.', 'they', 'also', 'have', 'some', 'type', 'of', 'rubber', 'feet', 'that', 'helps', 'with', 'leveling', 'the', 'stand', '.', 'these', 'are', 'pretty', 'impressive', '.', 'the', 'only', 'thing', 'that', 'i', 'do', \"n't\", 'like', 'is', 'that', 'the', 'platform', 'that', 'the', 'speakers', 'sit', 'on', 'are', 'a', 'bit', 'larger', 'than', 'i', \"'d\", 'like', '...', 'but', 'this', 'is', 'more', 'my', 'fault', 'for', 'not', 'reading', 'the', 'dimensions', '.', 'other', 'than', 'that', ',', 'i', 'highly', 'recommend', 'these', '.']\n",
            "\n",
            "Step 4: Remove stopwords\n",
            "['purchased', 'stands', 'hold', 'yamaha', 'hs5', \"'s\", '.', 'stands', 'really', 'well', 'made', '..', 'heavy', 'nice', 'look', '.', 'really', 'like', 'triangle', 'base', 'makes', 'easy', 'fit', 'corners', 'room', '.', 'locking', 'pins', 'great', 'keeping', 'stands', 'height', 'set', '.', 'also', 'type', 'rubber', 'feet', 'helps', 'leveling', 'stand', '.', 'pretty', 'impressive', '.', 'thing', \"n't\", 'like', 'platform', 'speakers', 'sit', 'bit', 'larger', \"'d\", 'like', '...', 'fault', 'reading', 'dimensions', '.', ',', 'highly', 'recommend', '.']\n",
            "\n",
            "Step 5: Remove punctuations\n",
            "['purchased', 'stands', 'hold', 'yamaha', 'hs5', 's', '', 'stands', 'really', 'well', 'made', '', 'heavy', 'nice', 'look', '', 'really', 'like', 'triangle', 'base', 'makes', 'easy', 'fit', 'corners', 'room', '', 'locking', 'pins', 'great', 'keeping', 'stands', 'height', 'set', '', 'also', 'type', 'rubber', 'feet', 'helps', 'leveling', 'stand', '', 'pretty', 'impressive', '', 'thing', 'nt', 'like', 'platform', 'speakers', 'sit', 'bit', 'larger', 'd', 'like', '', 'fault', 'reading', 'dimensions', '', '', 'highly', 'recommend', '']\n",
            "\n",
            "Step 6: Remove blank space token\n",
            "purchased stands hold yamaha hs5 s stands really well made heavy nice look really like triangle base makes easy fit corners room locking pins great keeping stands height set also type rubber feet helps leveling stand pretty impressive thing nt like platform speakers sit bit larger d like fault reading dimensions highly recommend\n",
            "\n",
            "\n",
            "Sample - File: file285.txt\n",
            "\n",
            "Step 1: Remove HTML tags\n",
            "Actually really like this piece...for what I needed it fits the bill. Buttons are solid, the fader works as it should and it's a solid build. Pay attention to the set up instructions, esp for Ableton and all will be fine. Very happy with my purchase.\n",
            "\n",
            "Step 2: Lowercase the text\n",
            "actually really like this piece...for what i needed it fits the bill. buttons are solid, the fader works as it should and it's a solid build. pay attention to the set up instructions, esp for ableton and all will be fine. very happy with my purchase.\n",
            "\n",
            "Step 3: Tokenization\n",
            "['actually', 'really', 'like', 'this', 'piece', '...', 'for', 'what', 'i', 'needed', 'it', 'fits', 'the', 'bill', '.', 'buttons', 'are', 'solid', ',', 'the', 'fader', 'works', 'as', 'it', 'should', 'and', 'it', \"'s\", 'a', 'solid', 'build', '.', 'pay', 'attention', 'to', 'the', 'set', 'up', 'instructions', ',', 'esp', 'for', 'ableton', 'and', 'all', 'will', 'be', 'fine', '.', 'very', 'happy', 'with', 'my', 'purchase', '.']\n",
            "\n",
            "Step 4: Remove stopwords\n",
            "['actually', 'really', 'like', 'piece', '...', 'needed', 'fits', 'bill', '.', 'buttons', 'solid', ',', 'fader', 'works', \"'s\", 'solid', 'build', '.', 'pay', 'attention', 'set', 'instructions', ',', 'esp', 'ableton', 'fine', '.', 'happy', 'purchase', '.']\n",
            "\n",
            "Step 5: Remove punctuations\n",
            "['actually', 'really', 'like', 'piece', '', 'needed', 'fits', 'bill', '', 'buttons', 'solid', '', 'fader', 'works', 's', 'solid', 'build', '', 'pay', 'attention', 'set', 'instructions', '', 'esp', 'ableton', 'fine', '', 'happy', 'purchase', '']\n",
            "\n",
            "Step 6: Remove blank space token\n",
            "actually really like piece needed fits bill buttons solid fader works s solid build pay attention set instructions esp ableton fine happy purchase\n",
            "\n",
            "\n",
            "Sample - File: file792.txt\n",
            "\n",
            "Step 1: Remove HTML tags\n",
            "It's perfect, although it's not the simplest of portable stands with its number of parts\n",
            "\n",
            "Step 2: Lowercase the text\n",
            "it's perfect, although it's not the simplest of portable stands with its number of parts\n",
            "\n",
            "Step 3: Tokenization\n",
            "['it', \"'s\", 'perfect', ',', 'although', 'it', \"'s\", 'not', 'the', 'simplest', 'of', 'portable', 'stands', 'with', 'its', 'number', 'of', 'parts']\n",
            "\n",
            "Step 4: Remove stopwords\n",
            "[\"'s\", 'perfect', ',', 'although', \"'s\", 'simplest', 'portable', 'stands', 'number', 'parts']\n",
            "\n",
            "Step 5: Remove punctuations\n",
            "['s', 'perfect', '', 'although', 's', 'simplest', 'portable', 'stands', 'number', 'parts']\n",
            "\n",
            "Step 6: Remove blank space token\n",
            "s perfect although s simplest portable stands number parts\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-ir91qz2t0_o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YcaITvUnt1CH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4nLLI041t1D8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Q2. Unigram Inverted Index and Phrase Queries**\n",
        "\n"
      ],
      "metadata": {
        "id": "1gfbjdB7t1UM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "import pickle\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from bs4 import BeautifulSoup\n",
        "import string"
      ],
      "metadata": {
        "id": "jOu49PF_u3Iq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def download_nltk_resources():\n",
        "    nltk.download('punkt')\n",
        "    nltk.download('stopwords')\n",
        "\n",
        "def get_data_path():\n",
        "    return \"/content/drive/MyDrive/Dataset3\"\n",
        "\n",
        "def clean_html(text):\n",
        "    soup_obj = BeautifulSoup(text, 'html.parser')\n",
        "    return soup_obj.get_text()\n",
        "\n",
        "def preprocess_text(text):\n",
        "    text = clean_html(text)\n",
        "    text = text.lower()\n",
        "    tokens = re.split(r'[;,\\s]+', text)\n",
        "    stop_words_set = set(stopwords.words('english'))\n",
        "    tokens = [token for token in tokens if token.lower() not in stop_words_set and token not in string.punctuation]\n",
        "    return tokens\n",
        "\n",
        "def create_inverted_index(data_path):\n",
        "    inverted_index = {}\n",
        "    for filename in os.listdir(data_path):\n",
        "        file_path = os.path.join(data_path, filename)\n",
        "        with open(file_path, 'r', encoding='utf-8') as file:\n",
        "            content = file.read()\n",
        "            tokens = preprocess_text(content)\n",
        "            for token in set(tokens):\n",
        "                inverted_index.setdefault(token, []).append(filename)\n",
        "    return inverted_index"
      ],
      "metadata": {
        "id": "pC2RqY9sRRH4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def save_inverted_index_as_text(inverted_index, file_path):\n",
        "    with open(file_path, 'w', encoding='utf-8') as file:\n",
        "        for term, postings in inverted_index.items():\n",
        "            file.write(f\"{term}: {', '.join(postings)}\\n\")\n",
        "\n",
        "def load_inverted_index(file_path):\n",
        "    with open(file_path, 'rb') as f:\n",
        "        return pickle.load(f)\n",
        "\n",
        "def delete_pickles():\n",
        "    try:\n",
        "        os.remove('unigramIndex')\n",
        "        os.remove('DocumentID')\n",
        "        print(\"Pickles deleted successfully.\")\n",
        "    except FileNotFoundError:\n",
        "        print(\"Pickles do not exist.\")"
      ],
      "metadata": {
        "id": "0P2CSQq4RRKm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def apply_and(result_set, next_file_set):\n",
        "    return result_set.intersection(next_file_set)\n",
        "\n",
        "def apply_or(result_set, next_file_set):\n",
        "    return result_set.union(next_file_set)\n",
        "\n",
        "def apply_and_not(result_set, next_file_set):\n",
        "    return result_set.difference(next_file_set)\n",
        "\n",
        "def apply_or_not(result_set, next_file_set, inverted_values_set):\n",
        "    return result_set.union(inverted_values_set.difference(next_file_set))\n",
        "\n",
        "def apply_operation(result_set, next_file_set, operation):\n",
        "    operations_dict = {\n",
        "        'AND': apply_and,\n",
        "        'OR': apply_or,\n",
        "        'AND NOT': apply_and_not,\n",
        "        'OR NOT': apply_or_not\n",
        "    }\n",
        "    return operations_dict[operation](result_set, next_file_set)"
      ],
      "metadata": {
        "id": "o0T6rQ-XRRNW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process_query(query, inverted_index, ops):\n",
        "    input_seq = preprocess_text(query)\n",
        "    file_sets = [set(inverted_index.get(term, [])) for term in input_seq]\n",
        "\n",
        "    result_set = file_sets[0]\n",
        "    for index, op in enumerate(ops.split(',')):\n",
        "        next_file_set = file_sets[index + 1] if index + 1 < len(file_sets) else set()\n",
        "        result_set = apply_operation(result_set, next_file_set, op)\n",
        "\n",
        "    return result_set\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    download_nltk_resources()\n",
        "    data_path = get_data_path()\n",
        "\n",
        "    inverted_index = create_inverted_index(data_path)\n",
        "\n",
        "    with open('inverted_index.pkl', 'wb') as f:\n",
        "        pickle.dump(inverted_index, f)\n",
        "\n",
        "    print(\"Inverted index created and saved.\")\n",
        "\n",
        "    inverted_values_set = set().union(*inverted_index.values())\n",
        "    inverted_index = load_inverted_index('inverted_index.pkl')\n",
        "\n",
        "    N = int(input(\"Enter the number of queries: \"))\n",
        "    queries = []\n",
        "    query_count = 0\n",
        "    while query_count < N:\n",
        "        query_input = input(\"Enter query: \")\n",
        "        operator_input = input(\"Enter operators (comma-separated): \")\n",
        "        queries.append((query_input, operator_input))\n",
        "        query_count += 1\n",
        "\n",
        "    for i, (query, ops) in enumerate(queries, start=1):\n",
        "        preprocessed_query = preprocess_text(query)\n",
        "        result_set = process_query(query, inverted_index, ops)\n",
        "        if result_set:\n",
        "            print(f\"Query {i}: {query}\")\n",
        "            print(\"Preprocessed Query:\", preprocessed_query)\n",
        "            print(\"Number of documents retrieved:\", len(result_set))\n",
        "            print(\"Names of the documents retrieved:\", ', '.join(result_set))\n",
        "        else:\n",
        "            print(f\"No documents retrieved for query {i}.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6w0vV0HZu3U3",
        "outputId": "32954187-1415-4fa3-9b9f-f0091556d2af"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Inverted index created and saved.\n",
            "Enter the number of queries: 1\n",
            "Enter query: rock band;\n",
            "Enter operators (comma-separated): OR\n",
            "Query 1: rock band;\n",
            "Preprocessed Query: ['rock', 'band']\n",
            "Number of documents retrieved: 35\n",
            "Names of the documents retrieved: preprocessed_file154.txt, preprocessed_file184.txt, preprocessed_file411.txt, preprocessed_file727.txt, preprocessed_file324.txt, preprocessed_file276.txt, preprocessed_file979.txt, preprocessed_file712.txt, preprocessed_file961.txt, preprocessed_file883.txt, preprocessed_file228.txt, preprocessed_file987.txt, preprocessed_file977.txt, preprocessed_file901.txt, preprocessed_file968.txt, preprocessed_file959.txt, preprocessed_file460.txt, preprocessed_file381.txt, preprocessed_file781.txt, preprocessed_file194.txt, preprocessed_file400.txt, preprocessed_file187.txt, preprocessed_file24.txt, preprocessed_file230.txt, preprocessed_file29.txt, preprocessed_file55.txt, preprocessed_file342.txt, preprocessed_file844.txt, preprocessed_file850.txt, preprocessed_file921.txt, preprocessed_file78.txt, preprocessed_file357.txt, preprocessed_file254.txt, preprocessed_file539.txt, preprocessed_file439.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wDYsXLCbtbcZ"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n"
      ],
      "metadata": {
        "id": "JAZtTjba1MN6"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DLk35TvKzCAq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### Q3. Positional Index and Phrase Queries"
      ],
      "metadata": {
        "id": "bD2THPrtzDmy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pickle\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from bs4 import BeautifulSoup\n",
        "from collections import defaultdict\n",
        "import string"
      ],
      "metadata": {
        "id": "2GNtKzixIjEg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def download_nltk_resources():\n",
        "    nltk.download('punkt')\n",
        "    nltk.download('stopwords')\n",
        "\n",
        "def get_data_path():\n",
        "    return \"/content/drive/MyDrive/Dataset3\""
      ],
      "metadata": {
        "id": "-r-ZaIXqFYIX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_text(text):\n",
        "    soup_obj = BeautifulSoup(text, 'html.parser')\n",
        "    text = soup_obj.get_text().lower()\n",
        "    tokens = word_tokenize(text)\n",
        "    stop_words_set = set(stopwords.words('english'))\n",
        "    tokens = [token for token in tokens if token.lower() not in stop_words_set - set(string.punctuation)]\n",
        "    return ' '.join(tokens)\n",
        "\n",
        "def preprocess_query(query):\n",
        "    query = query.lower()\n",
        "    tokens = word_tokenize(query)\n",
        "    stop_words_set = set(stopwords.words('english'))\n",
        "    tokens = [token for token in tokens if token.lower() not in stop_words_set]\n",
        "    tokens = [token for token in tokens if token not in string.punctuation]\n",
        "    tokens = [token for token in tokens if token.strip()]\n",
        "    return tokens"
      ],
      "metadata": {
        "id": "fZkbxngopnVZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_positional_index(documents, fnms):\n",
        "    positional_index = {token: {filename: [] for filename in fnms} for token in set(word_tokenize(' '.join(documents)))}\n",
        "    for doc_id, document in enumerate(documents):\n",
        "        tokens = word_tokenize(document)\n",
        "        for position, token in enumerate(tokens, start=1):\n",
        "            positional_index[token][fnms[doc_id]].append(position)\n",
        "    return positional_index"
      ],
      "metadata": {
        "id": "ou3_6yqfpnYv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def save_positional_index(positional_index,fnm):\n",
        "    with open(fnm,'wb') as file:\n",
        "        pickle.dump(positional_index, file)\n",
        "\n",
        "def load_positional_index(fnm):\n",
        "    with open(fnm,'rb') as file:\n",
        "        return pickle.load(file)"
      ],
      "metadata": {
        "id": "ql_-gkqRpqrj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def retrieve_documents_for_query(query_tokens, positional_index, fnms):\n",
        "    retrieved_documents = set()\n",
        "    for doc_filename in fnms:\n",
        "        doc_positions = [positional_index.get(token, {}).get(doc_filename, []) for token in query_tokens]\n",
        "        if all(doc_pos for doc_pos in doc_positions):\n",
        "            for pos in doc_positions[0]:\n",
        "                if all(pos + i in doc_pos for i, doc_pos in enumerate(doc_positions[1:], start=1)):\n",
        "                    retrieved_documents.add(doc_filename)\n",
        "    return retrieved_documents"
      ],
      "metadata": {
        "id": "i6-qknoCpquT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    download_nltk_resources()\n",
        "    data_path = get_data_path()\n",
        "    tfdf = [filename for filename in os.listdir(data_path) if filename.endswith('.txt')]\n",
        "    fnms = []\n",
        "\n",
        "    preprocessed_documents = []\n",
        "    i = 0\n",
        "    while i < len(tfdf):\n",
        "        fnm = tfdf[i]\n",
        "        file_path = os.path.join(data_path, fnm)\n",
        "        fnms.append(fnm)\n",
        "        with open(file_path, 'r', encoding='utf-8') as file:\n",
        "            preprocessed_text = file.read()\n",
        "            preprocessed_documents.append(preprocess_text(preprocessed_text))\n",
        "        i += 1\n",
        "\n",
        "    positional_index = build_positional_index(preprocessed_documents, fnms)\n",
        "    save_positional_index(positional_index, \"positional_index.pkl\")\n",
        "\n",
        "    loaded_positional_index = load_positional_index(\"positional_index.pkl\")\n",
        "\n",
        "    NoQuery= int(input(\"Enter number of queries: \"))\n",
        "    queries= [input(\"Enter query: \") for _ in range(NoQuery)]\n",
        "\n",
        "    for i, query in enumerate(queries,start=1):\n",
        "\n",
        "        query_tokens = preprocess_query(query)\n",
        "        retrieved_documents = retrieve_documents_for_query(query_tokens, loaded_positional_index, tfdf)\n",
        "        print(f\"Number of documents retrieved for query {i} using positional index: {len(retrieved_documents)}\")\n",
        "        print(f\"Names of documents retrieved for query {i} using positional index: {', '.join(retrieved_documents)}\")\n",
        "        print()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xNIOn6iRFYX5",
        "outputId": "dbe041f7-d049-4462-808e-0025d65adb0f"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter number of queries: 3\n",
            "Enter query: Rock;,baNd\n",
            "Enter query: eyes\n",
            "Enter query: bob\n",
            "Number of documents retrieved for query 1 using positional index: 2\n",
            "Names of documents retrieved for query 1 using positional index: preprocessed_file154.txt, preprocessed_file24.txt\n",
            "\n",
            "Number of documents retrieved for query 2 using positional index: 5\n",
            "Names of documents retrieved for query 2 using positional index: preprocessed_file923.txt, preprocessed_file847.txt, preprocessed_file626.txt, preprocessed_file213.txt, preprocessed_file758.txt\n",
            "\n",
            "Number of documents retrieved for query 3 using positional index: 0\n",
            "Names of documents retrieved for query 3 using positional index: \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GFZZGQWrFYcu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "ZfrM6w9PFYl9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}